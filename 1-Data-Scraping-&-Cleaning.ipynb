{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Kraabime veebilehelt PDFid ja slaidiesitlused maha\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import tika\n",
    "from tika import parser\n",
    "#Tegin siia meetodi, mis võtab nasdaqbalticu lehelt siis ISIN koodi ja kuupäeva järgi kõik .pdf formaadis failid,\n",
    "#laeb need lokaalselt mulle alla.\n",
    "def fetch_pdfs(ISIN,date):\n",
    "    url = f\"https://nasdaqbaltic.com/statistics/en/instrument/{ISIN}/reports?date={date}\"\n",
    "    #If there is no such folder, the script will create one automatically\n",
    "    global folder_location \n",
    "    folder_location = ISIN\n",
    "    basedir = r\"C:\\Users\\marek.keskull\\Documents\\webscraping\"\n",
    "    fold = os.path.join(basedir, folder_location)\n",
    "    print(\"The folder location that i am operating in: \" + fold)\n",
    "    if not os.path.exists(fold):os.mkdir(fold)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup= BeautifulSoup(response.text, \"html.parser\")\n",
    "    table = soup.find(\"tbody\")\n",
    "    #table_trs = table.find_all('tr')\n",
    "    links = table.select(\"a[href*='con']\")\n",
    "\n",
    "\n",
    "    for link in links:\n",
    "        filename = os.path.join(fold,link['href'].split('/')[-1])\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(requests.get(urljoin(url,link['href'])).content)\n",
    " #kasutan tika libraryt, selle peab panema lokaalselt käima eraldi, et kasutada tika rest teenust.   \n",
    " #Võtan kõik allalaetud PDF'id ning muudan need .txt failideks, et mul jääks alles ainult plain text\n",
    "    for root, dirs, files in os.walk(fold):\n",
    "        for file in files:\n",
    "            path_to_pdf = os.path.join(root, file)\n",
    "            [stem, ext] = os.path.splitext(path_to_pdf)\n",
    "            if ext == '.pdf':\n",
    "                print(\"Processing \" + path_to_pdf)\n",
    "                pdf_contents = parser.from_file(path_to_pdf)\n",
    "\n",
    "                path_to_txt = stem + '.txt'\n",
    "                with open(path_to_txt, 'w',encoding=\"utf-8\") as txt_file:\n",
    "                    print(\"Writing contents to \" + path_to_txt)\n",
    "                    txt_file.write(pdf_contents['content'])\n",
    "fetch_pdfs(\"EE3100073644\",\"2021-03-05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarters = []\n",
    "basedir = r\"C:\\Users\\marek.keskull\\Documents\\webscraping\"\n",
    "path = os.path.join(basedir, folder_location)\n",
    "files = os.listdir(path)\n",
    "for file in files:\n",
    "    path_to_pdf = os.path.join(path, file)\n",
    "    [stem, ext] = os.path.splitext(path_to_pdf)\n",
    "    if ext == '.txt':\n",
    "        quarters.append(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for c in quarters:\n",
    "    with open(path+ \"\\\\\" + c, \"rb\") as file:\n",
    "        text = file.read()\n",
    "        asd = text.strip()\n",
    "        decoded=str(asd,'utf-8')\n",
    "        data.append(decoded)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teen listidest dictionary\n",
    "ddata = dict(zip(quarters, data))\n",
    "ddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(list_of_text):\n",
    "    combined_text = ''.join(list_of_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = {key: [combine_text(value)] for (key, value) in ddata.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pandas dataframe data\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "data_df = pd.DataFrame.from_dict(data_combined).transpose()\n",
    "data_df.columns = ['text']\n",
    "data_df = data_df.sort_index()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Andmete puhastamine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Tekst kõik väikeste tähtedega\n",
    "- Eemdaldame ebavajaliku teksti, mis on kandiliste sulgude vahel\n",
    "- Eemaldame punktid\n",
    "- Eemaldame numbrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vaatame puhastatud teksti\n",
    "data_clean2 = pd.DataFrame(data_df.text.apply(round1))\n",
    "data_clean2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. round puhastamisele\n",
    "def clean_text_round2(text):\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\r', ' ', text)\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vaatame puhastatud teksti\n",
    "data_clean2 = pd.DataFrame(data_clean2.text.apply(round2))\n",
    "pd.options.display.max_colwidth = 500\n",
    "data_clean2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data_clean2.to_pickle(\"pickles/lhv2016t2020.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#moodustame document term matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(data_clean.text)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = data_clean.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_dtm.transpose()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 30 sõna sõnastikus\n",
    "top_dict = {}\n",
    "for c in data.columns:\n",
    "    top = data[c].sort_values(ascending=False).head(30)\n",
    "    top_dict[c]= list(zip(top.index, top.values))\n",
    "\n",
    "top_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data_dtm.to_pickle(\"pickles/historicallhvdtm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
