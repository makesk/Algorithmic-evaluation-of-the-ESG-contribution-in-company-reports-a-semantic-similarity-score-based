{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sõnade arvutamine\n",
    "\n",
    "Võtan tekstidest kõik sõnad ja võrdlen neid etteantud sõnadega, mis on seotud ESG'ga. Defineeritud sõnapaketi võtsin internetist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>unigrams</th>\n",
       "      <th>quarters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>annual_report_2019_luminor.txt</th>\n",
       "      <td>annual report luminor holding as consolidated...</td>\n",
       "      <td>[annual, report, luminor, holding, as, consoli...</td>\n",
       "      <td>annual_report_2019_luminor.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lhv_annual_2019.txt</th>\n",
       "      <td>group annual indd as lhv group consolidated an...</td>\n",
       "      <td>[group, annual, indd, as, lhv, group, consolid...</td>\n",
       "      <td>lhv_annual_2019.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seb_annual_report_2019.txt</th>\n",
       "      <td>annual reportannua l r e p o rt contents in br...</td>\n",
       "      <td>[annual, reportannua, l, r, e, p, o, rt, conte...</td>\n",
       "      <td>seb_annual_report_2019.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swedannual2019.txt</th>\n",
       "      <td>annual and sustainability report financial in...</td>\n",
       "      <td>[annual, and, sustainability, report, financia...</td>\n",
       "      <td>swedannual2019.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             text  \\\n",
       "annual_report_2019_luminor.txt   annual report luminor holding as consolidated...   \n",
       "lhv_annual_2019.txt             group annual indd as lhv group consolidated an...   \n",
       "seb_annual_report_2019.txt      annual reportannua l r e p o rt contents in br...   \n",
       "swedannual2019.txt               annual and sustainability report financial in...   \n",
       "\n",
       "                                                                         unigrams  \\\n",
       "annual_report_2019_luminor.txt  [annual, report, luminor, holding, as, consoli...   \n",
       "lhv_annual_2019.txt             [group, annual, indd, as, lhv, group, consolid...   \n",
       "seb_annual_report_2019.txt      [annual, reportannua, l, r, e, p, o, rt, conte...   \n",
       "swedannual2019.txt              [annual, and, sustainability, report, financia...   \n",
       "\n",
       "                                                      quarters  \n",
       "annual_report_2019_luminor.txt  annual_report_2019_luminor.txt  \n",
       "lhv_annual_2019.txt                        lhv_annual_2019.txt  \n",
       "seb_annual_report_2019.txt          seb_annual_report_2019.txt  \n",
       "swedannual2019.txt                          swedannual2019.txt  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "datacorp = pd.read_pickle('annualpickles/annualcorpuses.pkl')\n",
    "datacorp[\"unigrams\"] = datacorp[\"text\"].apply(nltk.word_tokenize)\n",
    "datacorp['quarters'] = datacorp.index\n",
    "datacorp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from re import sub\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim.similarities import SoftCosineSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# logimine\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)  # DEBUG # INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marek.keskull\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# stoppsõnad\n",
    "nltk.download('stopwords') \n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#andmetest võtan andmepealkirjad ja tekstid nimetan dokumentideks\n",
    "\n",
    "titles = [item for item in datacorp['quarters']]\n",
    "documents = [item for item in datacorp['text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    # puhastamine ja tokeniseerimine\n",
    "    doc = sub(r'<img[^<>]+(>|$)', \" image_token \", doc)\n",
    "    doc = sub(r'<[^<>]+(>|$)', \" \", doc)\n",
    "    doc = sub(r'\\[img_assist[^]]*?\\]', \" \", doc)\n",
    "    doc = sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \" url_token \", doc)\n",
    "    return [token for token in simple_preprocess(doc, min_len=0, max_len=float(\"inf\")) if token not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_string = 'biodiversity'\n",
    "query_Environment = 'Biodiversity Carbon Cleantech Clean Climate Coal Conservation Ecosystem Emission Energy Fuel Green Land Natural Pollution Renewable Resources Sustainability Sustainable Toxic Waste Water Accident Alcohol Anti-personnel Behavior Charity Community Controversial Controversy Discrimination Gambling Health Human capital Human rights Inclusion Injury Lab Munitions Opposition Pay Philanthropic Quality Responsible Advocacy Bribery Compensation Competitive Corruption Divestment Fraud GRI Independent Justice Stability Stewardship Transparency'\n",
    "query_Social = 'Accident Alcohol Anti-personnel Behavior Charity Community Controversial Controversy Discrimination Gambling Health Human capital Human rights Inclusion Injury Lab Munitions Opposition Pay Philanthropic Quality Responsible'\n",
    "query_Government = 'Advocacy Bribery Compensation Competitive Corruption Divestment Fraud GRI Independent Justice Stability Stewardship Transparency'\n",
    "\n",
    "\n",
    "# Preprocess meetod\n",
    "corpus = [preprocess(document) for document in documents]\n",
    "query = preprocess(query_Environment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# glove vektoripakk, siin on 400000 vektorit sees \n",
    "#https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "if 'glove' not in locals():  \n",
    "    glove = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "\n",
    "#arvutame välja koosinuse sarnasused välja sõnavektorides\n",
    "similarity_index = WordEmbeddingSimilarityIndex(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ehitame TF-idf mudeli\n",
    "def build_term(corpus, query):\n",
    "    # The search query must be in the dictionary as well, in case the terms do not overlap with the documents (we still want similarity)\n",
    "    dictionary = Dictionary(corpus+[query])\n",
    "    tfidf = TfidfModel(dictionary=dictionary)\n",
    "    # Create the term similarity matrix. \n",
    "    # The nonzero_limit enforces sparsity by limiting the number of non-zero terms in each column. \n",
    "    similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf)  # , nonzero_limit=None)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = build_term(corpus, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_similarity_scores(query,similarity_matrix):\n",
    "    dictionary = Dictionary(corpus+[query])\n",
    "    tfidf = TfidfModel(dictionary=dictionary)\n",
    "    query_tf = tfidf[dictionary.doc2bow(query)]\n",
    "    index = SoftCosineSimilarity(tfidf[[dictionary.doc2bow(document) for document in corpus]],similarity_matrix)\n",
    "    doc_similarity_scores = index[query_tf]\n",
    "    return doc_similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "document_sim_scores = doc_similarity_scores(query,tfidf_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meetod sarnasusskooride sortimiseks\n",
    "def sort_similarity_scores_by_document(doc_similarity_scores):\n",
    "    sorted_indexes = np.argsort(doc_similarity_scores)[::-1]\n",
    "    #for idx in sorted_indexes[:15]:\n",
    "        #print(f'{idx} \\t {doc_similarity_scores[idx]:0.3f} \\t {titles[idx]}')\n",
    "        \n",
    "    return sorted_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sim_scores = sort_similarity_scores_by_document(document_sim_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_similar_terms = []\n",
    "max_results_per_doc = 30\n",
    "for term in query:\n",
    "    dictionary = Dictionary(corpus+[query])\n",
    "    idx1 = dictionary.token2id[term]\n",
    "    for document in corpus:\n",
    "        results_this_doc = []\n",
    "        for word in set(document):\n",
    "            idx2 = dictionary.token2id[word]\n",
    "            score = tfidf_model.matrix[idx1, idx2]\n",
    "            if score > 0.0:\n",
    "                results_this_doc.append((word, score))\n",
    "        results_this_doc = sorted(results_this_doc, reverse=True, key=lambda x: x[1])  # sort results by score\n",
    "        results_this_doc = results_this_doc[:min(len(results_this_doc), max_results_per_doc)]  # take the top results\n",
    "        doc_similar_terms.append(results_this_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t 0.432 \t lhv_annual_2019.txt\n",
      "2 \t 0.487 \t seb_annual_report_2019.txt\n",
      "3 \t 0.518 \t swedannual2019.txt\n",
      "0 \t 0.462 \t annual_report_2019_luminor.txt\n"
     ]
    }
   ],
   "source": [
    "#esimese 15 dokumendi tulemused\n",
    "for idx in s[:15]:\n",
    "    similar_terms_string = ', '.join([result[0] for result in doc_similar_terms[idx]])\n",
    "    print(f'{idx} \\t {document_sim_scores[idx]:0.3f} \\t {titles[idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t 0.432 \t lhv_annual_2019.txt  :  biodiversity, conservation, ecological, sustainable, sustainability, preservation, diversity, environmental, resource, climate, impacts, environment, depletion, resources, natural, stewardship, development, footprint, awareness, mitigation, global, nature, agricultural, prevention, impact\n",
      "2 \t 0.487 \t seb_annual_report_2019.txt  :  ecosystem, conservation, ecosystems, sustainable, sustainability, diversity, environmental, resource, climate, impacts, environment, aquatic, resources, natural, forest, development, footprint, conducive, awareness, mitigation, global, nature, prosperity, prevention, pollution, impact, safeguarding\n",
      "3 \t 0.518 \t swedannual2019.txt  :  conservation, sustainable, sustainability, diversity, environmental, resource, climate, impacts, environment, resources, deforestation, natural, forest, preparedness, development, footprint, awareness, mitigation, global, nature, environments, agroforestry, prosperity, prevention, impact, safeguarding\n",
      "0 \t 0.462 \t annual_report_2019_luminor.txt  :  ecological, sustainable, sustainability, diversity, environmental, resource, environment, resources, natural, stewardship, preserve, utilization, development, footprint, awareness, mitigation, global, nature, prevention, impact, safeguarding\n"
     ]
    }
   ],
   "source": [
    "#esimese 15 dokumendi tulemused koos sarnaste sõnadega\n",
    "results = []\n",
    "for idx in s[:15]:\n",
    "    similar_terms_string = ', '.join([result[0] for result in doc_similar_terms[idx]])\n",
    "    results.append([idx,document_sim_scores[idx],titles[idx],similar_terms_string])\n",
    "    print(f'{idx} \\t {document_sim_scores[idx]:0.3f} \\t {titles[idx]}  :  {similar_terms_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results, columns=[\"Index of document\", \"Similarity score\",\"Document name\",\"Most similar words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Similarity score</th>\n",
       "      <th>Document name</th>\n",
       "      <th>Most similar words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.431862</td>\n",
       "      <td>lhv_annual_2019.txt</td>\n",
       "      <td>biodiversity, conservation, ecological, sustai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.486714</td>\n",
       "      <td>seb_annual_report_2019.txt</td>\n",
       "      <td>ecosystem, conservation, ecosystems, sustainab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.518059</td>\n",
       "      <td>swedannual2019.txt</td>\n",
       "      <td>conservation, sustainable, sustainability, div...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.461859</td>\n",
       "      <td>annual_report_2019_luminor.txt</td>\n",
       "      <td>ecological, sustainable, sustainability, diver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index  Similarity score                   Document name  \\\n",
       "0      1          0.431862             lhv_annual_2019.txt   \n",
       "1      2          0.486714      seb_annual_report_2019.txt   \n",
       "2      3          0.518059              swedannual2019.txt   \n",
       "3      0          0.461859  annual_report_2019_luminor.txt   \n",
       "\n",
       "                                  Most similar words  \n",
       "0  biodiversity, conservation, ecological, sustai...  \n",
       "1  ecosystem, conservation, ecosystems, sustainab...  \n",
       "2  conservation, sustainable, sustainability, div...  \n",
       "3  ecological, sustainable, sustainability, diver...  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "df.to_pickle('annualresults/2019banksimilarityscores.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order of similarity</th>\n",
       "      <th>Similarity score</th>\n",
       "      <th>Document name</th>\n",
       "      <th>Most similar words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.264879</td>\n",
       "      <td>swedbankQ2_20_eng.txt</td>\n",
       "      <td>critical, fax, bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.267326</td>\n",
       "      <td>swedbankQ3_20_eng.txt</td>\n",
       "      <td>critical, fax, bank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Order of similarity  Similarity score          Document name  \\\n",
       "0                    1          0.264879  swedbankQ2_20_eng.txt   \n",
       "1                    2          0.267326  swedbankQ3_20_eng.txt   \n",
       "\n",
       "    Most similar words  \n",
       "0  critical, fax, bank  \n",
       "1  critical, fax, bank  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datacorpswed = pd.read_pickle('resultdata/swedsimilarityscores.pkl')\n",
    "datacorpswed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order of similarity</th>\n",
       "      <th>Similarity score</th>\n",
       "      <th>Document name</th>\n",
       "      <th>Most similar words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.261196</td>\n",
       "      <td>lhv2020_q1_en_eur_con_00.txt</td>\n",
       "      <td>note, collection, operation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.040926</td>\n",
       "      <td>lhv2020_q2_en_eur_00_00.txt</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.267063</td>\n",
       "      <td>lhv2020_q2_en_eur_con_00.txt</td>\n",
       "      <td>note, collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.040926</td>\n",
       "      <td>lhv2020_q1_en_eur_00_00.txt</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Order of similarity  Similarity score                 Document name  \\\n",
       "0                    1          0.261196  lhv2020_q1_en_eur_con_00.txt   \n",
       "1                    2          0.040926   lhv2020_q2_en_eur_00_00.txt   \n",
       "2                    3          0.267063  lhv2020_q2_en_eur_con_00.txt   \n",
       "3                    0          0.040926   lhv2020_q1_en_eur_00_00.txt   \n",
       "\n",
       "            Most similar words  \n",
       "0  note, collection, operation  \n",
       "1                               \n",
       "2             note, collection  \n",
       "3                               "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datacorplhv = pd.read_pickle('resultdata/lhvsimilarityscores.pkl')\n",
    "datacorplhv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
