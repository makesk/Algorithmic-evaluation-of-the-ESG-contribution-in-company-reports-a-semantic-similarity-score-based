{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sõnade arvutamine\n",
    "\n",
    "Võtan tekstidest kõik sõnad ja võrdlen neid etteantud sõnadega, mis on seotud ESG'ga. Defineeritud sõnapaketi võtsin internetist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "datacorp = pd.read_pickle('pickles/Seb2016t2020.pkl')\n",
    "datacorp['document'] = datacorp.index\n",
    "\n",
    "query_G = 'Audit and control, Board structure, Remuneration, Shareholder rights, Transparency and Performance'\n",
    "query_S = 'Access to medicines, HIV, AIDs, Nutrition, Product safety, Community relations, Privacy and free expression, Security, Weak, governance zones, Diversity, Health and safety, ILO core conventions, Supply chain labor standards, Bribery and corruption, Political influence, Responsible marketing, Whistle-blowing systems, disclosure and reporting, Governance of sustainability issues, Stakeholder engagement, UNGC compliance'\n",
    "query_E = 'Biofuels, Climate ,Emissions ,land, Biodiversity, Water, Environmental, standards, Pollution, Supply, Waste, recycling'\n",
    "datacorp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from re import sub\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim.similarities import SoftCosineSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# logimine\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)  # DEBUG # INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# stoppsõnad\n",
    "nltk.download('stopwords') \n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#andmetest võtan andmepealkirjad ja tekstid nimetan dokumentideks\n",
    "\n",
    "titles = [item for item in datacorp['document']]\n",
    "documents = [item for item in datacorp['text']]\n",
    "\n",
    "\n",
    "print(f'{len(documents)} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    logging.info( 'tokenizing' )\n",
    "    doc = doc.lower().split()\n",
    "    doc = [w for w in doc if w not in stopwords]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import string\n",
    "def preprocess_query(doc):\n",
    "    logging.info( 'tokenizing' )\n",
    "    doc = doc.lower().split()\n",
    "    doc = [remove_punc(i) for i in doc]\n",
    "    doc = [w for w in doc if w not in stopwords]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(string):\n",
    "    punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n",
    "    for ele in string:  \n",
    "        if ele in punc:  \n",
    "            string = string.replace(ele, \"\") \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [preprocess(document) for document in documents]\n",
    "\n",
    "query = preprocess_query(query_G)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#pre-trained embeddings\n",
    "# glove vektoripakk, siin on 400000 vektorit sees \n",
    "#https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "if 'glove' not in locals():  \n",
    "    glove = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "#print(glove.most_similar(\"remuneration\"))\n",
    "\n",
    "#A term similarity index that computes cosine similarities between word embeddings.\n",
    "#1) Compute cosine similarities between word embeddings.\n",
    "#2) Retrieve the closest word embeddings (by cosine similarity) to a given word embedding.\n",
    "\n",
    "similarity_index = WordEmbeddingSimilarityIndex(glove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vecs = similarity_index.keyedvectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from vec2graph import visualize\n",
    "\n",
    "#visualize(r'C:\\Users\\marek.keskull\\Documents\\GitHub\\NLP\\Vizualization', vecs, 'audit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove vektorite koosinussarnasus indeksi näide\n",
    "#most_similar = similarity_index.keyedvectors.most_similar(positive=['water'], topn=10)\n",
    "#most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ehitame TF-idf mudeli\n",
    "    \n",
    "#kõigepealt ehitame valmis andmesõnastiku, kus on sees kõik dokumendi sõnad ja otsingupäringu sõnad vormis: 'võti':'sõna'\n",
    "logging.info( 'building dictionary' )\n",
    "dictionary = Dictionary(corpus+[query])\n",
    "\n",
    "#This module implements functionality related to the \n",
    "#Term Frequency - Inverse Document Frequency vector space bag-of-words models.\n",
    "tfidf = TfidfModel(dictionary=dictionary)\n",
    "\n",
    "  \n",
    "#Builds a sparse term similarity matrix using a term similarity index. \n",
    "similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf, nonzero_limit=100) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary = Dictionary(corpus+[query])\n",
    "#print(dictionary[429])\n",
    "#basedir =r\"C:\\Users\\marek.keskull\\Documents\\GitHub\\NLP\\Vizualization\"\n",
    "#logging.info( 'saving dictionary' )\n",
    "#dictFile = basedir + '.dict'\n",
    "#dictionary.save_as_text(dictFile, sort_by_word=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse maatriksi kasutamise näide\n",
    "#similarity_matrix.inner_product(dictionary.doc2bow(query),dictionary.doc2bow(corpus[7]))\n",
    "#similarity_matrix.matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix.matrix.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dictionary)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for document in tfidf[[dictionary.doc2bow(document) for document in corpus]]:\n",
    " #   print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute soft cosine similarity against a corpus of documents by storing the index matrix in memory.\n",
    "index = SoftCosineSimilarity(tfidf[[dictionary.doc2bow(document) for document in corpus]],similarity_matrix)\n",
    "\n",
    "doc_similarity_scores = index.get_similarities(dictionary.doc2bow(query))\n",
    "doc_similarity_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indexes = np.argsort(doc_similarity_scores)[::-1]\n",
    "d = []\n",
    "for idx in sorted_indexes:\n",
    "    d.append(\n",
    "        {\n",
    "            'Document no': idx,\n",
    "            'Similarity score with query': doc_similarity_scores[idx],\n",
    "            'Document name': titles[idx]\n",
    "        }\n",
    "    )\n",
    "\n",
    "final = pd.DataFrame(d)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc_similar_terms = []\n",
    "max_results_per_doc = 30\n",
    "#query = ['audit', 'control', 'board', 'structure', 'remuneration', 'shareholder', 'rights', 'transparency', 'performance']\n",
    "for term in query:\n",
    "    #dictionary = Dictionary(corpus+[query])\n",
    "    #dictionary is query + my corpus(which has 25 documents)\n",
    "\n",
    "    idx1 = dictionary.token2id[term]\n",
    "    for document in corpus:\n",
    "        #print(document.name)\n",
    "        results_this_doc = []\n",
    "        for word in set(document):\n",
    "            idx2 = dictionary.token2id[word]\n",
    "            score = similarity_matrix.matrix[idx1, idx2]\n",
    "            if score > 0.0:\n",
    "                results_this_doc.append((word, score))\n",
    "               \n",
    "        results_this_doc = sorted(results_this_doc, reverse=True, key=lambda x: x[1])\n",
    "        \n",
    "        results_this_doc = results_this_doc[:min(len(results_this_doc), max_results_per_doc)]\n",
    "        #print(results_this_doc)\n",
    "        doc_similar_terms.append(results_this_doc)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "for idx in sorted_indexes[:30]:\n",
    "    similar_terms_string = ', '.join([result[0] for result in doc_similar_terms[idx]])\n",
    "    results.append(\n",
    "        {\n",
    "            'Document no': idx,\n",
    "            'Similarity score with query': doc_similarity_scores[idx],\n",
    "            'Document name': titles[idx],\n",
    "            \"Most similar words\":similar_terms_string\n",
    "        }\n",
    "    )\n",
    "\n",
    "similar_words = pd.DataFrame(results)\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "similar_words.to_pickle('Governanceresults/SEB.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
