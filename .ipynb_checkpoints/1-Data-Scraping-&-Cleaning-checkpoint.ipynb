{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kraabime veebilehelt PDFid ja slaidiesitlused maha\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "#Tegin siia meetodi, mis võtab nasdaqbalticu lehelt siis ISIN koodi ja kuupäeva järgi kõik .pdf formaadis failid,\n",
    "#laeb need lokaalselt mulle alla.\n",
    "def fetch_pdfs(ISIN,date):\n",
    "    url = f\"https://nasdaqbaltic.com/statistics/en/instrument/{ISIN}/reports?date={date}\"\n",
    "    #If there is no such folder, the script will create one automatically\n",
    "    folder_location = r'C:\\Users\\marek.keskull\\webscraping\\lhv'\n",
    "    if not os.path.exists(folder_location):os.mkdir(folder_location)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup= BeautifulSoup(response.text, \"html.parser\")\n",
    "    tr = soup.find(\"tbody\")\n",
    "    pdf2020 = tr.find(\"tr\")\n",
    "    \n",
    "    for link in pdf2020.select(\"a[href$='.pdf']\"):\n",
    "        filename = os.path.join(folder_location,link['href'].split('/')[-1])\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(requests.get(urljoin(url,link['href'])).content)\n",
    "fetch_pdfs(\"EE3100073644\",\"2021-01-11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kasutan tika libraryt, selle peab panema lokaalselt käima eraldi, et kasutada tika rest teenust.\n",
    "import tika\n",
    "from tika import parser\n",
    "\n",
    "import os\n",
    "\n",
    "folder_location = r'C:\\Users\\marek.keskull\\webscraping\\coop'\n",
    "#Võtan kõik allalaetud PDF'id ning muudan need .txt failideks, et mul jääks alles ainult plain text\n",
    "def extract_text_from_pdfs_recursively(dir):\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            path_to_pdf = os.path.join(root, file)\n",
    "            [stem, ext] = os.path.splitext(path_to_pdf)\n",
    "            if ext == '.pdf':\n",
    "                print(\"Processing \" + path_to_pdf)\n",
    "                pdf_contents = parser.from_file(path_to_pdf)\n",
    "                path_to_txt = stem + '.txt'\n",
    "                with open(path_to_txt, 'w',encoding=\"utf-8\") as txt_file:\n",
    "                    print(\"Writing contents to \" + path_to_txt)\n",
    "                    txt_file.write(pdf_contents['content'])\n",
    "                    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_text_from_pdfs_recursively(folder_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = r'C:\\Users\\marek.keskull\\webscraping\\ignitis'\n",
    "files = os.listdir(path)\n",
    "print(files)\n",
    "for file in files:\n",
    "    path_to_pdf = os.path.join(path, file)\n",
    "    [stem, ext] = os.path.splitext(path_to_pdf)\n",
    "    if ext == '.txt':\n",
    "        name , extension = path_to_pdf.split('ignitis\\\\')\n",
    "        os.rename(os.path.join(path, file), os.path.join(path,'ignitis' + extension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarters = []\n",
    "path = r'C:\\Users\\marek.keskull\\webscraping\\ignitis'\n",
    "files = os.listdir(path)\n",
    "for file in files:\n",
    "    path_to_pdf = os.path.join(path, file)\n",
    "    [stem, ext] = os.path.splitext(path_to_pdf)\n",
    "    if ext == '.txt':\n",
    "        quarters.append(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for c in quarters:\n",
    "    with open(path+ \"\\\\\" + c, \"rb\") as file:\n",
    "        text = file.read()\n",
    "        asd = text.strip()\n",
    "        decoded=str(asd,'utf-8')\n",
    "        data.append(decoded)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#teen listidest dictionary\n",
    "ddata = dict(zip(quarters, data))\n",
    "ddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(list_of_text):\n",
    "    combined_text = ''.join(list_of_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = {key: [combine_text(value)] for (key, value) in ddata.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas dataframe data\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "data_df = pd.DataFrame.from_dict(data_combined).transpose()\n",
    "data_df.columns = ['text']\n",
    "data_df = data_df.sort_index()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#vaatan datat \n",
    "data_df.text.loc['ignitis2020_q2_en_eur_con_ias.txt']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Andmete puhastamine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Tekst kõik väikeste tähtedega\n",
    "- Eemaldame punktid\n",
    "- Eemaldame numbrid\n",
    "- Eemdaldame ebavajaliku teksti, mis on kandiliste sulgude vahel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vaatame puhastatud teksti\n",
    "data_clean = pd.DataFrame(data_df.text.apply(round1))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.text.loc['ignitis2020_q2_en_eur_con_ias.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. round puhastamisele\n",
    "def clean_text_round2(text):\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    #text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    #text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('\\r', ' ', text)\n",
    "    #text = re.sub(r\"\\s+\",\" \", text)\n",
    "    #text = re.sub(r\"^\\s+\", \"\", text)\n",
    "    #text = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", text)\n",
    "    #text = re.sub(r'/\\d\\.\\s+|[a-z]\\)\\s+|•\\s+|[A-Z]\\.\\s+|[IVX]+\\.\\s+/g', \"\",text)\n",
    "    #text = re.sub(r'\\xc2\\xb7',\"\",text)\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vaatame puhastatud teksti\n",
    "data_clean = pd.DataFrame(data_clean.text.apply(round2))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.text.loc['ignitis2020_q2_en_eur_con_ias.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#moodustame document term matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(data_clean.text)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = data_clean.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_dtm.transpose()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 30 sõna sõnastikus\n",
    "top_dict = {}\n",
    "for c in data.columns:\n",
    "    top = data[c].sort_values(ascending=False).head(30)\n",
    "    top_dict[c]= list(zip(top.index, top.values))\n",
    "\n",
    "top_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data_dtm.to_pickle(\"pickles/ignitisdtm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.to_pickle('pickles/ignitisdata_clean.pkl')\n",
    "pickle.dump(cv, open(\"pickles/ignitiscv.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
